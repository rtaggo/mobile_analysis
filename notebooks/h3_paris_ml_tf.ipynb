{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codes.bq_utils import bq_query_to_dataframe\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = 'LIMIT 15000'\n",
    "limits=''\n",
    "sql_events = f\"\"\"\n",
    "WITH geovisits_h3_indexes AS (\n",
    "    SELECT *, `carto-os-eu`.h3.LONGLAT_ASH3(longitude, latitude, 10) h3_idx\n",
    "    FROM `ggo-ppos-bqgis.singlespot.geovisits_paris_matview`\n",
    ")\n",
    "select sptId, uuid, latitude, longitude, accuracy, eventId,  \n",
    "arrival, departure, score, rank, category, feature, placeName, h3_idx \n",
    "FROM geovisits_h3_indexes {limits}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bq_query_to_dataframe(sql_events)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sptId'] = df['sptId'].astype(str)\n",
    "df['category'] = df['category'].astype(str)\n",
    "df['feature'] = df['feature'].astype(str)\n",
    "df['arrival'] = pd.to_datetime(df['arrival'], format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "df['depature'] = pd.to_datetime(df['departure'], format='%Y-%m-%d %H:%M:%S', utc=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = pd.DataFrame(df[['h3_idx', 'category', 'feature']])\n",
    "df_w['count'] = 1\n",
    "df_w.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_columns = ['category', 'feature']\n",
    "grouped_columns = ['category']\n",
    "\n",
    "h3_pivot = pd.DataFrame(pd.pivot_table(df_w,  columns=grouped_columns, index='h3_idx', values=\"count\", aggfunc=[len], fill_value=0, margins = True, margins_name='total')).reset_index()\n",
    "h3_pivot.columns = h3_pivot.columns.map('_'.join).str.replace('len_', '')\n",
    "\n",
    "h3_pivot.rename(columns={ h3_pivot.columns[0]: \"h3_idx\" }, inplace=True)\n",
    "h3_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "indexNames = h3_pivot[h3_pivot['h3_idx'] == 'total'].index\n",
    "h3_pivot.drop(indexNames, inplace=True)\n",
    "# h3_pivot[h3_pivot['h3_idx'] == 'total']\n",
    "# h3_pivot.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = h3_pivot.columns\n",
    "df_names = df_names[1:]\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "h3_pivot_corr=h3_pivot[df_names]\n",
    "correlations= h3_pivot_corr.corr()\n",
    "mask = np.zeros_like(correlations)  # make mask\n",
    "mask[np.triu_indices_from(mask)] = True  # mask the upper triangle\n",
    "fig, ax = plt.subplots(figsize=(11, 9))  # create a figure and a subplot\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)  # custom color map\n",
    "\"\"\"\n",
    "sns.heatmap(\n",
    "    correlations,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    linewidth=0.5,\n",
    "    cbar_kws={'shrink': 0.5}\n",
    ")\n",
    "\"\"\"\n",
    "sns.heatmap(\n",
    "    correlations,\n",
    "    annot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h3_pivot_corr.drop('total',axis=1)\n",
    "y = h3_pivot_corr['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('std_scalar', StandardScaler())\n",
    "])\n",
    "X_train= pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOOGLE TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codes.tf_model_1 as tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs = 400\n",
    "batch_size = 30\n",
    "\n",
    "# Specify the feature and the label.\n",
    "my_feature = \"shop\"  # the total number of rooms on a specific city block.\n",
    "my_label=\"total\" # the median value of a house on a specific city block.\n",
    "# That is, you're going to create a model that predicts house value based \n",
    "# solely on total_rooms.  \n",
    "\n",
    "# Discard any pre-existing version of the model.\n",
    "my_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the functions.\n",
    "my_model = tf1.build_model(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "pd_y_train = pd.DataFrame(y_train, columns=['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias, epochs, rmse = tf1.train_model(my_model, pd_X_train[my_feature], pd_y_train[my_label], epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
    "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
    "\n",
    "#tf1.plot_the_model(trained_weight=weight, trained_bias=bias, feature=my_feature, label=my_label)\n",
    "tf1.plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val(model):\n",
    "    pred = cross_val_score(model, X, y, cv=10)\n",
    "    return pred.mean()\n",
    "\n",
    "def print_evaluate(true, predicted):  \n",
    "    mae = metrics.mean_absolute_error(true, predicted)\n",
    "    mse = metrics.mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
    "    r2_square = metrics.r2_score(true, predicted)\n",
    "    print('MAE:', mae)\n",
    "    print('MSE:', mse)\n",
    "    print('RMSE:', rmse)\n",
    "    print('R2 Square', r2_square)\n",
    "    print('__________________________________')\n",
    "    \n",
    "def evaluate(true, predicted):\n",
    "    mae = metrics.mean_absolute_error(true, predicted)\n",
    "    mse = metrics.mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
    "    r2_square = metrics.r2_score(true, predicted)\n",
    "    return mae, mse, rmse, r2_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression(normalize=True)\n",
    "lin_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lin_reg.predict(X_test)\n",
    "train_pred = lin_reg.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodust Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample Consensus - RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1, \n",
    "              precompute=True, \n",
    "#               warm_start=True, \n",
    "              positive=True, \n",
    "              selection='random',\n",
    "              random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_2_d = poly_reg.fit_transform(X_train)\n",
    "X_test_2_d = poly_reg.transform(X_test)\n",
    "\n",
    "lin_reg = LinearRegression(normalize=True)\n",
    "lin_reg.fit(X_train_2_d,y_train)\n",
    "\n",
    "test_pred = lin_reg.predict(X_test_2_d)\n",
    "train_pred = lin_reg.predict(X_train_2_d)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\n",
    "sgd_reg.fit(X_train, y_train)\n",
    "\n",
    "test_pred = sgd_reg.predict(X_test)\n",
    "train_pred = sgd_reg.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "print('====================================')\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artficial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(0.00001), loss='mse')\n",
    "\n",
    "r = model.fit(X_train, y_train,\n",
    "              validation_data=(X_test,y_test),\n",
    "              batch_size=1,\n",
    "              epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=1000)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "test_pred = rf_reg.predict(X_test)\n",
    "train_pred = rf_reg.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\n",
    "svm_reg.fit(X_train, y_train)\n",
    "\n",
    "test_pred = svm_reg.predict(X_test)\n",
    "train_pred = svm_reg.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "print('Train set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_train, train_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n",
    "                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.set_index('Model', inplace=True)\n",
    "results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d4b87a31be3be5552b9d089024709dbfa8582e38000e3c08e4b675c3dfaa523"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
